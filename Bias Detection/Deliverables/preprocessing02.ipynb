{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceab75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b54a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# !pip uninstall certifi -y\n",
    "# !pip install --no-cache-dir certifi==2025.7.14\n",
    "\n",
    "from textacy import preprocessing as tprep\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article Dataframe\n",
    "article_df = pd.read_csv('/Users/amalkurian/Desktop/Dissertation/Bias Detection/Deliverables/matching_articles04.csv')\n",
    "print(f'List all the columns{list(article_df.columns)}')\n",
    "# Events Datafrane\n",
    "events_df = pd.read_csv('/Users/amalkurian/Desktop/Dissertation/Bias Detection/diffbot-export-tigray-war.csv')\n",
    "print(f'List all the columns{list(events_df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ddda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['Actions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c407f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['Load_Date'] = pd.to_datetime(article_df['Load_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa67019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def generate_event_id(row):\n",
    "    raw_String = f\"{row['date_str']}_{row['title']}_{row['author']}\"\n",
    "    return hashlib.sha256(raw_String.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "events_df['event_id'] = events_df.progress_apply(generate_event_id, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eab18c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005bc24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text) # gets the entities in the text\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Events Dataframe: Extracting Entities using Spacy\n",
    "# Character count has to be less than 1 million to avoid memory issues\n",
    "\n",
    "def safe_extract(text):\n",
    "    if isinstance(text, str) and len(text) < 1_000_000:\n",
    "        return extract_entities(text)\n",
    "    return []\n",
    "\n",
    "\n",
    "events_df['entities'] = events_df['text'].progress_apply(safe_extract)\n",
    "# Explode the entities For Events Dataframe\n",
    "events_entities_long = events_df[['event_id', 'entities']].explode('entities')\n",
    "# # Extract entity text and label\n",
    "events_entities_long[['entity_text', 'label']] = pd.DataFrame(\n",
    "    events_entities_long['entities'].tolist(), index=events_entities_long.index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d49f1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d960b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e604195",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_entities = (events_entities_long\n",
    "        .groupby('event_id')\n",
    "        .agg({\n",
    "            'entity_text': lambda x: list(x),\n",
    "            'label': lambda x: list(x)\n",
    "        })\n",
    "        .rename(columns={\n",
    "            'entity_text': 'entities_Group', 'label': 'labels_Group'})\n",
    "        )\n",
    "\n",
    "events_df = events_df.merge(grouped_entities, on='event_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e17c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = events_df.rename(columns = {'label_Group_y': 'label_Group'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c09e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the date extracted into DateTime format %Y-%m-%d\n",
    "\n",
    "events_df['date_str'] = (\n",
    "    events_df['date_str']\n",
    "    .str.replace(r'^d','',regex=True)\n",
    ")\n",
    "\n",
    "events_df['date_str'] = events_df['date_str'].str.split('T').str[0] # str is an accessor for string operations on Series\n",
    "events_df['date_str'] = pd.to_datetime(events_df['date_str'], format='%Y-%m-%d', errors='coerce')\n",
    "events_df['date_str'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7546200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "def flatten_entities(col):\n",
    "    flattened = []\n",
    "    for ents in col:\n",
    "        if not isinstance(ents, list):\n",
    "            continue  # Skip NaNs or weird types\n",
    "        for item in ents:\n",
    "            if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "                ent, label = item\n",
    "                if label in {\"ORG\", \"GPE\", \"PERSON\", \"NORP\", \"LOC\"}:\n",
    "                    flattened.append(ent)\n",
    "    return flattened\n",
    "\n",
    "# Flatten the entities and count occurances for Article Dataframe   \n",
    "article_df['entities'] = article_df['entities'].apply(ast.literal_eval) \n",
    "all_entities = flatten_entities(article_df['entities'])\n",
    "\n",
    "# Flatten the entities and count occurrences for Events Dataframe\n",
    "all_events_entities = flatten_entities(events_df['entities'])\n",
    "\n",
    "entity_counter = Counter(all_entities)\n",
    "entity_events_counter = Counter(all_events_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b3bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_df['entities'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae1b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Entities From Article Dataframe\n",
    "threshold = 10\n",
    "raw_ethiopia_entities = ((entity, count) for entity, count in entity_counter.most_common() if count >= threshold)\n",
    "raw_events_entities = ((entity, count) for entity, count in entity_events_counter.most_common() if count >= threshold)\n",
    "raw_events_entities = list(raw_events_entities)\n",
    "raw_ethiopia_entities = list(raw_ethiopia_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "def mapping_events_entities(raw_entities):\n",
    "    events_entities = list(e[0].lower() for e in raw_entities)\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    print('Encoding Entities...') \n",
    "    events_embeddings = model.encode(events_entities, show_progress_bar = True)\n",
    "    events_similarity_matrix = cosine_similarity(events_embeddings)\n",
    "\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters = None,\n",
    "        metric = 'precomputed',\n",
    "        linkage = 'complete',\n",
    "        distance_threshold = 0.3)\n",
    "    \n",
    "    events_distance_matrix = 1 - events_similarity_matrix\n",
    "    events_distance_matrix = events_distance_matrix.astype(np.float64)  # Convert to float32 for HDBSCAN compatibility\n",
    "    labels = clustering.fit_predict(events_distance_matrix)  # position of the entities in the similarity/distance matrix closer the entity, smaller the distance\n",
    "\n",
    "    events_cluster_df = pd.DataFrame({'entity': events_entities, 'cluster': labels}) # labels are used to create a list of similar entities and the most common entities will be extracted for the canonical term\n",
    "    events_canonical_entities = {}\n",
    "\n",
    "    for cluster_id, group in events_cluster_df.groupby('cluster'):\n",
    "        # Choose the most common name as canonical\n",
    "        canonical = Counter(group['entity']).most_common(1)[0][0]\n",
    "        # canonical = sorted(group['entity'], key=lambda x: len(x))[0]\n",
    "        for ent in group['entity']:\n",
    "            events_canonical_entities[ent] = canonical\n",
    "    \n",
    "    mapping_events_entities = [(ent, events_canonical_entities[ent]) for ent in events_entities]\n",
    "    return mapping_events_entities #tuple\n",
    "\n",
    "\n",
    "# Generate the mapping for events entities\n",
    "mapping_lookup_events_entities = mapping_events_entities(raw_events_entities)\n",
    "for original, canonical in mapping_lookup_events_entities[:500]:\n",
    "    print(f\"{original} → {canonical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_keywords(text):\n",
    "    doc = nlp(text)\n",
    "    keyphrases = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if len(chunk.text.split()) > 1:  # Only consider multi-word phrases\n",
    "            keyphrases.add(chunk.text.lower())\n",
    "\n",
    "    verbs = {token.lemma_ for token in doc if token.pos_ == 'VERB' and token.lemma_ not in stopwords.words('english')}\n",
    "    return list(keyphrases), list(verbs)\n",
    "\n",
    "article_df[['Key_Phrases', 'Actions']] = article_df['cleaned_content'].progress_apply(lambda x: pd.Series(extract_keywords(x) if isinstance(x, str) else ([], [])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df['Actions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "raw_ethiopia_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ebca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_lookup_articles_entities = mapping_events_entities(raw_ethiopia_entities)\n",
    "for original, canonical in mapping_lookup_articles_entities[:500]:\n",
    "    print(f\"{original} → {canonical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bfe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_articles_entities_dictionary = dict(mapping_lookup_articles_entities)\n",
    "mapped_events_entities_dictionary = dict(mapping_lookup_events_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe88c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the entities in the articles and events Dataframe\n",
    "# !pip install rapidfuzz\n",
    "\n",
    "from rapidfuzz import process\n",
    "\n",
    "def normalize_entity(ent, entity_mapping, threshold=90): # for spelling mistakes for canonical entities\n",
    "    if not isinstance(ent, str) or not ent.strip():\n",
    "        return ent\n",
    "    match, score, _ = process.extractOne(\n",
    "        ent.lower(), entity_mapping.keys()\n",
    "    )\n",
    "\n",
    "    return entity_mapping[match] if match else ent \n",
    "\n",
    "\n",
    "# Events Entities Normalization\n",
    "normalized_events_entities = [normalize_entity(ent,mapped_events_entities_dictionary) for ent,_ in raw_events_entities]\n",
    "normalized_events_entities[:50]  # Display first 50 normalized entities\n",
    "# Normalize the entities in the articles \n",
    "normalized_articles_entities = [normalize_entity(ent,mapped_articles_entities_dictionary) for ent,_ in raw_ethiopia_entities]\n",
    "normalized_articles_entities[:50]  # Display first 50 normalized entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d266f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_entities_long['normalized'] = events_entities_long.progress_apply(lambda x: normalize_entity(x['entity_text'], mapped_events_entities_dictionary), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c46b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explode the entities\n",
    "entities_long = article_df[['article_id', 'entities']].explode('entities')\n",
    "\n",
    "# # Extract entity text and label\n",
    "entities_long[['entity_text', 'label']] = pd.DataFrame(\n",
    "    entities_long['entities'].tolist(), index=entities_long.index\n",
    "    )\n",
    "entities_long.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_long['normalized'] = entities_long.progress_apply(lambda x: normalize_entity(x['entity_text'], mapped_articles_entities_dictionary), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2147640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_types = ['PERSON', 'ORG', 'NORP']\n",
    "geo_types = ['GPE', 'LOC']\n",
    "actor_df = entities_long[entities_long['label'].isin(actor_types)] \\\n",
    "    .groupby('article_id')['normalized'].apply(set).reset_index(name='actor_entities')\n",
    "geo_df = entities_long[entities_long['label'].isin(geo_types)] \\\n",
    "    .groupby('article_id')['normalized'].apply(set).reset_index(name='geo_entities')\n",
    "# Merge actor and geo entities\n",
    "merged_df = pd.merge(actor_df, geo_df, on='article_id', how='outer')\n",
    "# Merge cleanly\n",
    "article_df = article_df.merge(merged_df, on='article_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a90069",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_types = ['PERSON', 'ORG', 'NORP']\n",
    "geo_types = ['GPE', 'LOC']\n",
    "\n",
    "actor_df = events_entities_long[events_entities_long['label'].isin(actor_types)] \\\n",
    "    .groupby('event_id')['normalized'].apply(set).reset_index(name='actor_entities')\n",
    "\n",
    "geo_df = events_entities_long[events_entities_long['label'].isin(geo_types)] \\\n",
    "    .groupby('event_id')['normalized'].apply(set).reset_index(name='geo_entities')\n",
    "\n",
    "# Merge actor and geo entities\n",
    "merged_df = pd.merge(actor_df, geo_df, on='event_id', how='outer')\n",
    "\n",
    "\n",
    "# Merge cleanly\n",
    "events_df = events_df.merge(merged_df, on='event_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15af08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77b6e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b3b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter events for each article based on the date range\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "def filter_events_for_article(Article_date):\n",
    "    \n",
    "    lower = Article_date - timedelta(days=27)\n",
    "    upper = Article_date + timedelta(days=7) \n",
    "    filtered = events_df[events_df['date_str'].between(lower, upper)] # it is a DataFrame\n",
    "    return filtered['event_id'].tolist()\n",
    "\n",
    "\n",
    "article_df['Load_Date'] = pd.to_datetime(article_df['Load_Date'])\n",
    "article_df['Load_Date'].head()\n",
    "\n",
    "article_df['matching_events'] = article_df['Load_Date'].progress_apply(filter_events_for_article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5b0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25965316",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b6ad53",
   "metadata": {},
   "source": [
    "# Entity Similarity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5b0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_set(x):\n",
    "    if isinstance(x, set):\n",
    "        return x\n",
    "    if isinstance(x, list):\n",
    "        return set(x)\n",
    "    if pd.isna(x) or not x:\n",
    "        return set()\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            # Safely parse string literal like \"{a, b, c}\" into set\n",
    "            parsed = ast.literal_eval(x)\n",
    "            # parsed might be set, list, tuple — convert to set\n",
    "            return set(parsed)\n",
    "        except (ValueError, SyntaxError):\n",
    "            # Fallback: split by commas if parsing fails\n",
    "            return set(e.strip() for e in x.strip('{}').split(',') if e.strip())\n",
    "    return set([x])\n",
    "\n",
    "def compute_entity_similarity(article_actors, article_geos, event_actors, event_geos):\n",
    "    # Sanitize inputs\n",
    "    article_actors = safe_set(article_actors)\n",
    "    event_actors = safe_set(event_actors)\n",
    "    article_geos = safe_set(article_geos)\n",
    "    event_geos = safe_set(event_geos)\n",
    "    \n",
    "    actor_score = (\n",
    "        len(article_actors & event_actors) / len(event_actors | article_actors)\n",
    "        if event_actors else 0\n",
    "    )\n",
    "    geo_score = (\n",
    "        len(article_geos & event_geos) / len(event_geos | article_geos)\n",
    "        if event_geos else 0\n",
    "    )\n",
    "    return actor_score, geo_score\n",
    "\n",
    "similarity_scores = []\n",
    "\n",
    "# Convert events_df into a quick lookup dict for speed\n",
    "event_lookup = {\n",
    "    row['event_id']: row\n",
    "    for _, row in events_df.iterrows()\n",
    "}\n",
    "\n",
    "for _, article_row in article_df.iterrows():\n",
    "    article_id = article_row['article_id']\n",
    "    a_actors = article_row['actor_entities']\n",
    "    a_geos = article_row['geo_entities']\n",
    "    \n",
    "    # Get pre-filtered event IDs for this article\n",
    "    matching_events = article_row.get('matching_events', [])\n",
    "\n",
    "    for event_id in matching_events:\n",
    "        # Defensive check: skip if event_id not in lookup\n",
    "        \n",
    "        if event_id not in event_lookup:\n",
    "            continue\n",
    "\n",
    "        event_row = event_lookup[event_id]\n",
    "        e_actors = event_row['actor_entities']\n",
    "        e_geos = event_row['geo_entities']\n",
    "        \n",
    "        # Compute similarity\n",
    "        actor_score, geo_score = compute_entity_similarity(a_actors, a_geos, e_actors, e_geos)\n",
    "        print(actor_score)\n",
    "        similarity_scores.append({\n",
    "            'article_id': article_id,\n",
    "            'event_id': event_id,\n",
    "            'actor_score': actor_score,\n",
    "            'geo_score': geo_score,\n",
    "            'combined_score': (actor_score + geo_score) / 2  # or weighted\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7031a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find_combined_score = pd.DataFrame(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158924aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find_combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Cosine Similarity\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")  # or 'bge-large-en-v1.5'\n",
    "\n",
    "# article_df['cleaned_content']\n",
    "# article_df['matching_events'] = list of event_ids within same time window\n",
    "# events_df['event_id'], events_df['text']\n",
    "\n",
    "\n",
    "# Event embedding index (for fast lookup)\n",
    "events_df['title'] = events_df['title'].fillna('').astype(str)\n",
    "events_id_to_vector = {\n",
    "    row['event_id']: model.encode(row['title'], convert_to_tensor=True, batch_size=32, show_progress_bar=False)\n",
    "    for _, row in events_df.iterrows()\n",
    "}\n",
    "\n",
    "# --- Function to embed and average article sentences\n",
    "def embed_article_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return torch.zeros(model.get_sentence_embedding_dimension())\n",
    "    sentence_embeddings = model.encode(sentences, convert_to_tensor=True, batch_size=32, show_progress_bar=False)\n",
    "    return torch.mean(sentence_embeddings, dim=0) # gets you a single embedding for the whole paragraph by finding the mean\n",
    "\n",
    "# compare only against its matching_events\n",
    "article_match_scores = []\n",
    "article_match_ids = []\n",
    "\n",
    "for idx, row in article_df.iterrows():\n",
    "    article_vec = embed_article_sentences(row.title)\n",
    "\n",
    "    matching_event_ids = row.get('matching_events', [])\n",
    "    if not matching_event_ids:\n",
    "        article_match_scores.append(None)\n",
    "        article_match_ids.append(None)\n",
    "        continue\n",
    "\n",
    "    # Get event vectors for those in the matching time window\n",
    "    event_vecs = [events_id_to_vector[eid] for eid in matching_event_ids if eid in events_id_to_vector]\n",
    "    \n",
    "    if not event_vecs:\n",
    "        article_match_scores.append(None)\n",
    "        article_match_ids.append(None)\n",
    "        continue\n",
    "\n",
    "    event_tensor = torch.stack(event_vecs) # create a 2D matrix for cosine similarity, it won't work on a list\n",
    "\n",
    "    # Cosine similarity\n",
    "    sim_scores = util.cos_sim(article_vec, event_tensor).squeeze(0)  # Shape: [num_events]\n",
    "    \n",
    "    scores_list = sim_scores.tolist()\n",
    "\n",
    "    article_match_scores.append(scores_list)\n",
    "    article_match_ids.append(matching_event_ids)\n",
    "\n",
    "# Attach to DataFrame\n",
    "article_df['match_score_Title'] = article_match_scores\n",
    "article_df['event_similarity_id_Title'] = article_match_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5d17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc6f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # Fill NaNs\n",
    "# article_df['title'] = article_df['title'].fillna('').astype(str)\n",
    "# events_df['title'] = events_df['title'].fillna('').astype(str)\n",
    "\n",
    "# # Keep mapping from event_id to title\n",
    "# event_id_to_title = events_df.set_index('event_id')['title'].to_dict()\n",
    "\n",
    "# # TF-IDF vectorizer\n",
    "# vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# # Combine all relevant titles for fitting\n",
    "# all_titles = list(article_df['title'].unique()) + list(events_df['title'].unique())\n",
    "# vectorizer.fit(all_titles)\n",
    "\n",
    "# # 1️⃣ Embed article titles\n",
    "# article_tfidf = vectorizer.transform(article_df['title'].tolist())\n",
    "\n",
    "# # Prepare storage\n",
    "# match_scores = []\n",
    "# match_ids = []\n",
    "\n",
    "# # 2️⃣ Loop over articles\n",
    "# for idx, row in article_df.iterrows():\n",
    "#     matching_event_ids = row.get('matching_events', [])\n",
    "#     if not matching_event_ids:\n",
    "#         match_scores.append(None)\n",
    "#         match_ids.append(None)\n",
    "#         continue\n",
    "\n",
    "#     # Get titles for the matching events\n",
    "#     event_titles = [event_id_to_title[eid] for eid in matching_event_ids if eid in event_id_to_title]\n",
    "#     if not event_titles:\n",
    "#         match_scores.append(None)\n",
    "#         match_ids.append(None)\n",
    "#         continue\n",
    "\n",
    "#     event_tfidf = vectorizer.transform(event_titles)\n",
    "\n",
    "#     # Cosine similarity\n",
    "#     article_vec = article_tfidf[idx]\n",
    "#     sim_scores = cosine_similarity(article_vec, event_tfidf).flatten()\n",
    "    \n",
    "#     match_scores.append(sim_scores.tolist())\n",
    "#     match_ids.append([matching_event_ids[i] for i, _ in enumerate(event_titles)])\n",
    "\n",
    "# # Attach to DataFrame\n",
    "# article_df['match_score_Title'] = match_scores\n",
    "# article_df['event_similarity_id_Title'] = match_ids\n",
    "\n",
    "# print(article_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17947c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_df = article_df.drop(columns = {'match_score_Title', 'event_similarity_id_Title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30993e92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6febf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f03645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode article_df lists into one row per event_id and score\n",
    "exploded = article_df.explode(['event_similarity_id_Title', 'match_score_Title']).rename(columns={\n",
    "    'event_similarity_id_Title': 'event_id',\n",
    "    'match_score_Title': 'match_score'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80824dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find_combined_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d646a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the lists into rows for clean merging\n",
    "exploded_article = article_df.explode(['event_similarity_id_Title', 'match_score_Title']).rename(columns={\n",
    "    'event_similarity_id_Title': 'event_id_2',\n",
    "    'match_score_Title': 'match_score'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e481c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploded_article = exploded_article.rename(columns = {\n",
    "#     'event_id': 'event_id_old',\n",
    "# })\n",
    "\n",
    "# exploded_article.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e038903",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_article = exploded_article.rename(columns = {\n",
    "    'event_id_2': 'event_id',\n",
    "})\n",
    "\n",
    "exploded_article.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Keep only the first occurrence of each column name\n",
    "# exploded_article = exploded_article.loc[:, exploded_article.columns.duplicated()]\n",
    "\n",
    "# print(exploded_article.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1a5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_find_combined_score.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03401a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = exploded_article.columns.tolist()\n",
    "first_idx = cols.index('match_score')\n",
    "# Rename the first one\n",
    "cols[first_idx] = 'match_score_old'\n",
    "exploded_article.columns = cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d931bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Merge on both article_id and event_id\n",
    "merged_df = pd.merge(\n",
    "    to_find_combined_score,\n",
    "    exploded_article[['article_id', 'event_id', 'match_score']],\n",
    "    on=['article_id', 'event_id'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(merged_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_article.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f782f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03838d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Convert to datetime if needed\n",
    "article_df['Load_Date'] = pd.to_datetime(article_df['Load_Date'])\n",
    "events_df['date_str'] = pd.to_datetime(events_df['date_str'])\n",
    "\n",
    "# Params from the paper\n",
    "alpha_h = 1.0  # for historical\n",
    "alpha_p = 0.8  # for predictive\n",
    "lambda_ = 0.8\n",
    "max_past_days = 30\n",
    "max_future_days = 10\n",
    "\n",
    "# Convert events_df to a dict for fast lookup\n",
    "event_date_lookup = dict(zip(events_df['event_id'], events_df['date_str']))\n",
    "\n",
    "# Function to calculate temporal proximity \n",
    "# Distance from 0 is considered over negative or positive value\n",
    "def compute_exp_temporal_score(pub_date, event_date):\n",
    "    delta_days = (event_date - pub_date).days  # Positive if event after article (predictive)\n",
    "    \n",
    "    if delta_days > 0:\n",
    "        # Predictive reporting Kind of optional categorization in this context\n",
    "        decay = delta_days / max_future_days\n",
    "        score = alpha_p * np.exp(-lambda_ * decay)\n",
    "    else:\n",
    "        # Historical reporting\n",
    "        decay = abs(delta_days) / max_past_days\n",
    "        score = alpha_h * np.exp(-lambda_ * decay)\n",
    "    \n",
    "    return round(score, 4)\n",
    "\n",
    "def compute_log_temporal_score(pub_date, event_date):\n",
    "    delta_days = abs((event_date - pub_date).days)\n",
    "\n",
    "    if delta_days > 0:\n",
    "        H = 27\n",
    "    else:\n",
    "        H = 7\n",
    "\n",
    "    value = (delta_days + 1)/H\n",
    "    inner = math.log(value)\n",
    "    if inner <= 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    score = -math.log(H * inner) / math.log(H)\n",
    "    return round(score, 4)\n",
    "\n",
    "# Apply to all article-event pairs\n",
    "def compute_temporal_scores(row):\n",
    "    pub_date = row['Load_Date']\n",
    "    matching_events = row.get('matching_events', [])\n",
    "\n",
    "    exp_scores = {}\n",
    "    log_scores = {}\n",
    "\n",
    "    for eid in matching_events:\n",
    "        event_date = event_date_lookup.get(eid)\n",
    "\n",
    "        if pd.notna(event_date):\n",
    "            exp_score = compute_exp_temporal_score(pub_date, event_date)\n",
    "            exp_scores[eid] = exp_score\n",
    "\n",
    "            log_score = compute_log_temporal_score(pub_date, event_date)\n",
    "            log_scores[eid] = log_score\n",
    "\n",
    "    return {'exp': exp_scores, 'log': log_scores}\n",
    "\n",
    "# Add temporal proximity scores to article_df\n",
    "article_df['temporal_scores'] = article_df.apply(compute_temporal_scores, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2696b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Merge the temporal_scores into the main df\n",
    "merged_with_temp = merged_df.merge(article_df[['article_id', 'temporal_scores']], on='article_id', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199642a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_exp(row):\n",
    "    exp_dict = row['temporal_scores'].get('exp', {})\n",
    "    return exp_dict.get(row['event_id'], np.nan)\n",
    "\n",
    "merged_with_temp['temporal_score_exp'] = merged_with_temp.apply(extract_exp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b360a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_temp = merged_with_temp.drop(columns=['temporal_scores'])\n",
    "merged_with_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0242a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_temp['temporal_score_exp'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b96f75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_with_temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220abc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def normalize_score(score, min_val, max_val):\n",
    "    return (score - min_val) / (max_val - min_val) if max_val > min_val else 0\n",
    "\n",
    "def combined_score(row, weights, min_vals, max_vals):\n",
    "    entity_ = normalize_score(row.get('combined_score', 0), min_vals['entity_similarity'], max_vals['entity_similarity'])\n",
    "    temporal_ = normalize_score(row.get('temporal_score_exp', 0), min_vals['temporal_exp_score'], max_vals['temporal_exp_score'])\n",
    "    semantic_ = normalize_score(row.get('match_score', 0), min_vals['text_similarity'], max_vals['text_similarity'])\n",
    "    \n",
    "    combined = (\n",
    "        weights['semantic'] * semantic_ + \n",
    "        weights['entity'] * entity_ +\n",
    "        weights['temporal'] * temporal_\n",
    "    )\n",
    "    return combined\n",
    "\n",
    "\n",
    "def classify_match(score, threshold=0.45):\n",
    "    return \"Valid\" if score >= threshold else \"Invalid\"\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'semantic': 0.4,\n",
    "    'entity': 0.4,\n",
    "    'temporal': 0.2\n",
    "}\n",
    "\n",
    "cols_to_numeric = ['match_score', 'combined_score', 'temporal_score_exp']\n",
    "for col in cols_to_numeric:\n",
    "    merged_with_temp[col] = pd.to_numeric(merged_with_temp[col], errors='coerce')\n",
    "\n",
    "\n",
    "min_vals = {\n",
    "    'text_similarity': merged_with_temp['match_score'].min(),\n",
    "    'entity_similarity': merged_with_temp['combined_score'].min(),\n",
    "    'temporal_exp_score': merged_with_temp['temporal_score_exp'].min()\n",
    "}\n",
    "\n",
    "max_vals = {\n",
    "    'text_similarity': merged_with_temp['match_score'].max(),\n",
    "    'entity_similarity': merged_with_temp['combined_score'].max(),\n",
    "    'temporal_exp_score': merged_with_temp['temporal_score_exp'].max()\n",
    "}\n",
    "\n",
    "merged_with_temp['final_scores'] = merged_with_temp.progress_apply(\n",
    "    lambda row: combined_score(row, weights, min_vals, max_vals), axis=1)\n",
    "\n",
    "merged_with_temp['match_Labels'] = merged_with_temp['final_scores'].progress_apply(classify_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95918393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article_id, find the max final_scores\n",
    "max_scores = merged_with_temp.groupby('article_id')['final_scores'].transform('max')\n",
    "\n",
    "# Filter rows where final_scores == max for that article_id\n",
    "filtered_df = merged_with_temp[merged_with_temp['final_scores'] == max_scores].copy()\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32d6cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['match_Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_keep_order(lst):\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for item in lst:\n",
    "        if item not in seen:\n",
    "            seen.add(item)\n",
    "            deduped.append(item)\n",
    "    return deduped\n",
    "\n",
    "combined_df = filtered_df.groupby('article_id').agg({\n",
    "    'event_id': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'actor_score': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'geo_score': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'combined_score': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'match_score': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'temporal_score_exp': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'final_scores': lambda x: deduplicate_keep_order(list(x)),\n",
    "    'match_Labels': lambda x: deduplicate_keep_order(list(x))\n",
    "}).reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cd27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85096938",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['match_Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56bfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and bring over match_labels and any other columns from combined_df\n",
    "article_df_merged = article_df.merge(combined_df, on='article_id', how='left')\n",
    "# Fill 'match_labels' with 'Invalid' where it's NaN (i.e., no match from combined_df)\n",
    "article_df_merged['match_Labels'] = article_df_merged['match_Labels'].fillna('Invalid')\n",
    "article_df_merged['match_Labels'] = article_df_merged['match_Labels'].astype(str).str.strip(\"[]\").str.strip(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d123430",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged  = article_df_merged.drop(columns=['match_Labels_x',\n",
    "       'actor_entities_y', 'geo_entities_y', 'event_id_x', 'actor_score_x',\n",
    "       'geo_score_x', 'combined_score_x', 'match_score_x',\n",
    "       'temporal_score_exp_x', 'final_scores_y', 'match_Labels_y'\n",
    "])\n",
    "\n",
    "article_df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c834bd6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0e33f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c731410",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged['match_Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4094815",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged.to_csv(\"article_event_matches_mergedIII.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d1ee3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac359c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df.to_csv(\"events03_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# article_df = pd.read_csv('article_event_matches_mergedII.csv')\n",
    "article_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df = (\n",
    "    article_df.rename(columns={c: c[:-2] for c in article_df.columns if c.endswith(\"_y\")})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_csv('events03_df.csv')\n",
    "events_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged['event_id_y'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197f3b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_event_id_from_max_index(row):\n",
    "#     idx = int(row['max_score_index'])\n",
    "#     events_ids = row['event_similarity_id']\n",
    "#     if events_ids and isinstance(events_ids, list) and  0 <= idx < len(events_ids):\n",
    "#         return events_ids[idx]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# article_df_merged['Top_event_id'] = article_df_merged.apply(get_event_id_from_max_index, axis=1)\n",
    "# # Create the mapping dictionary once\n",
    "event_id_to_title = dict(zip(events_df['event_id'], events_df['title']))\n",
    "\n",
    "# Define a function that maps only if match_label is Valid\n",
    "def map_titles_if_valid(row):\n",
    "    if row['match_Labels'] == 'Valid' and isinstance(row['event_id_y'], list):\n",
    "        return [event_id_to_title.get(eid) for eid in row['event_id_y']]\n",
    "    return None\n",
    "\n",
    "article_df_merged['Top_event_titles'] = article_df_merged.apply(map_titles_if_valid, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79091e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
